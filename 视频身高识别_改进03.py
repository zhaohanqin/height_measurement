import cv2
import os
import numpy as np
from ultralytics import YOLO

# æ¨¡å‹åŠ è½½
model = YOLO("yolov8n.pt")

# æ ¡å‡†æ–‡ä»¶è·¯å¾„
calibration_file = "calibration.txt"
reference_face_file = "reference_face.jpg"

# A4çº¸æ ‡å‡†å°ºå¯¸ï¼ˆå˜ç±³ï¼‰
A4_WIDTH = 21.0
A4_HEIGHT = 29.7

# å‚è€ƒå‚æ•°
REFERENCE_HEIGHT = 170  # å‚è€ƒèº«é«˜ï¼ˆå˜ç±³ï¼‰
REFERENCE_DISTANCE = 200  # å‚è€ƒè·ç¦»ï¼ˆå˜ç±³ï¼‰

# åŠ è½½äººè„¸æ£€æµ‹å™¨
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# åˆå§‹åŒ–ç‰¹å¾æ£€æµ‹å™¨
sift = cv2.SIFT_create()
bf = cv2.BFMatcher()

# å‚è€ƒäººè„¸ç‰¹å¾
reference_face_features = None

def load_or_create_reference_face():
    """åŠ è½½æˆ–åˆ›å»ºå‚è€ƒäººè„¸ç‰¹å¾"""
    global reference_face_features
    
    if os.path.exists(reference_face_file):
        # åŠ è½½å‚è€ƒå›¾ç‰‡
        ref_img = cv2.imread(reference_face_file)
        ref_gray = cv2.cvtColor(ref_img, cv2.COLOR_BGR2GRAY)
        # æ£€æµ‹äººè„¸
        faces = face_cascade.detectMultiScale(ref_gray, 1.1, 5)
        if len(faces) > 0:
            x, y, w, h = faces[0]
            face_roi = ref_gray[y:y+h, x:x+w]
            # æå–ç‰¹å¾
            keypoints, descriptors = sift.detectAndCompute(face_roi, None)
            if descriptors is not None:
                reference_face_features = descriptors
                print("âœ… å·²åŠ è½½å‚è€ƒäººè„¸ç‰¹å¾")
                return True
    else:
        print("âš ï¸ æœªæ‰¾åˆ°å‚è€ƒäººè„¸å›¾ç‰‡ï¼Œè¯·æä¾›ä¸€å¼ æ¸…æ™°çš„æ­£é¢ç…§ç‰‡")
        # æ‰“å¼€æ‘„åƒå¤´æ‹ç…§
        cap = cv2.VideoCapture(0)
        while True:
            ret, frame = cap.read()
            if not ret:
                break
                
            # æ˜¾ç¤ºé¢„è§ˆ
            cv2.imshow("Take Reference Photo (Press SPACE to capture)", frame)
            
            # æŒ‰ç©ºæ ¼é”®æ‹ç…§
            if cv2.waitKey(1) & 0xFF == ord(' '):
                # ä¿å­˜å›¾ç‰‡
                cv2.imwrite(reference_face_file, frame)
                print("âœ… å·²ä¿å­˜å‚è€ƒç…§ç‰‡")
                break
        
        cap.release()
        cv2.destroyAllWindows()
        
        # å¤„ç†æ–°ä¿å­˜çš„ç…§ç‰‡
        ref_img = cv2.imread(reference_face_file)
        ref_gray = cv2.cvtColor(ref_img, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(ref_gray, 1.1, 5)
        if len(faces) > 0:
            x, y, w, h = faces[0]
            face_roi = ref_gray[y:y+h, x:x+w]
            keypoints, descriptors = sift.detectAndCompute(face_roi, None)
            if descriptors is not None:
                reference_face_features = descriptors
                print("âœ… å·²æå–å‚è€ƒäººè„¸ç‰¹å¾")
                return True
    
    print("âŒ æ— æ³•æå–å‚è€ƒäººè„¸ç‰¹å¾")
    return False

def detect_face(frame):
    """æ£€æµ‹äººè„¸å¹¶è¿”å›ä½ç½®"""
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(
        gray,
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(30, 30)
    )
    return faces

def is_reference_face(face_roi):
    """åˆ¤æ–­æ˜¯å¦åŒ¹é…å‚è€ƒäººè„¸"""
    if reference_face_features is None:
        return False
        
    # æå–å½“å‰äººè„¸ç‰¹å¾
    keypoints, descriptors = sift.detectAndCompute(face_roi, None)
    if descriptors is None:
        return False
        
    # ç‰¹å¾åŒ¹é…
    matches = bf.knnMatch(reference_face_features, descriptors, k=2)
    
    # åº”ç”¨æ¯”ç‡æµ‹è¯•
    good_matches = []
    for m, n in matches:
        if m.distance < 0.75 * n.distance:
            good_matches.append(m)
            
    # å¦‚æœå¥½çš„åŒ¹é…ç‚¹è¶³å¤Ÿå¤šï¼Œè®¤ä¸ºæ˜¯åŒä¸€ä¸ªäºº
    return len(good_matches) > 10

def detect_a4_paper(frame):
    """æ£€æµ‹A4çº¸å¹¶è¿”å›å…¶è½®å»“å’Œå˜æ¢çŸ©é˜µ"""
    # è½¬æ¢ä¸ºç°åº¦å›¾
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    # é«˜æ–¯æ¨¡ç³Š
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)
    # è¾¹ç¼˜æ£€æµ‹
    edges = cv2.Canny(blurred, 50, 150)
    # æŸ¥æ‰¾è½®å»“
    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # æŒ‰é¢ç§¯æ’åº
    contours = sorted(contours, key=cv2.contourArea, reverse=True)
    
    for contour in contours:
        # è®¡ç®—è½®å»“é¢ç§¯
        area = cv2.contourArea(contour)
        if area < 1000:  # å¿½ç•¥å¤ªå°çš„è½®å»“
            continue
            
        # å¤šè¾¹å½¢è¿‘ä¼¼
        peri = cv2.arcLength(contour, True)
        approx = cv2.approxPolyDP(contour, 0.02 * peri, True)
        
        # ç¡®ä¿æ˜¯å››è¾¹å½¢
        if len(approx) == 4:
            # è®¡ç®—é•¿å®½æ¯”
            rect = cv2.minAreaRect(contour)
            width = min(rect[1])
            height = max(rect[1])
            aspect_ratio = height / width
            
            # A4çº¸çš„é•¿å®½æ¯”çº¦ä¸º1.414
            if 1.3 < aspect_ratio < 1.5:
                # è·å–å››ä¸ªè§’ç‚¹
                points = approx.reshape(4, 2)
                
                # è®¡ç®—ç›®æ ‡å°ºå¯¸ï¼ˆåƒç´ ï¼‰
                target_width = int(width)
                target_height = int(height)
                
                # å®šä¹‰ç›®æ ‡ç‚¹ï¼ˆç”¨äºé€è§†å˜æ¢ï¼‰
                dst_points = np.array([
                    [0, 0],
                    [target_width - 1, 0],
                    [target_width - 1, target_height - 1],
                    [0, target_height - 1]
                ], dtype="float32")
                
                # è®¡ç®—é€è§†å˜æ¢çŸ©é˜µ
                M = cv2.getPerspectiveTransform(points.astype("float32"), dst_points)
                
                return contour, M, target_width, target_height
    
    return None, None, None, None

def calibrate_with_a4(frame):
    """ä½¿ç”¨A4çº¸è¿›è¡Œæ ‡å®š"""
    contour, M, target_width, target_height = detect_a4_paper(frame)
    
    if contour is not None:
        # è®¡ç®—åƒç´ åˆ°å˜ç±³çš„æ¯”ä¾‹
        pixel_to_cm_ratio_width = A4_WIDTH / target_width
        pixel_to_cm_ratio_height = A4_HEIGHT / target_height
        
        # ä½¿ç”¨å¹³å‡å€¼ä½œä¸ºæœ€ç»ˆæ¯”ä¾‹
        pixel_to_cm_ratio = (pixel_to_cm_ratio_width + pixel_to_cm_ratio_height) / 2
        
        # ä¿å­˜æ ‡å®šç»“æœ
        with open(calibration_file, "w", encoding="utf-8") as f:
            f.write(str(pixel_to_cm_ratio))
            
        return True, pixel_to_cm_ratio, contour
    
    return False, None, None

# è¯»å–æˆ–åˆå§‹åŒ–æ¢ç®—æ¯”ä¾‹
if os.path.exists(calibration_file):
    with open(calibration_file, "r", encoding="utf-8") as f:
        pixel_to_cm_ratio = float(f.read())
    print(f"ğŸ“‚ å·²åŠ è½½æ ‡å®šå€¼ï¼š1 åƒç´  â‰ˆ {pixel_to_cm_ratio:.4f} cm")
    calibrated = True
else:
    pixel_to_cm_ratio = None
    calibrated = False
    print("âš ï¸ è¯·å°†A4çº¸æ”¾åœ¨ç”»é¢ä¸­ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨å®Œæˆæ ‡å®šã€‚")

# åŠ è½½å‚è€ƒäººè„¸ç‰¹å¾
face_recognized = load_or_create_reference_face()

def estimate_distance(bbox_width):
    """æ ¹æ®æ£€æµ‹æ¡†å®½åº¦ä¼°ç®—è·ç¦»"""
    if bbox_width == 0:
        return None
    # ä½¿ç”¨ç›¸ä¼¼ä¸‰è§’å½¢åŸç†ä¼°ç®—è·ç¦»
    estimated_distance = (REFERENCE_HEIGHT * REFERENCE_DISTANCE) / (pixel_to_cm_ratio * bbox_width)
    return estimated_distance

def adjust_height_ratio(distance):
    """æ ¹æ®è·ç¦»è°ƒæ•´èº«é«˜æ¯”ä¾‹"""
    if distance is None:
        return pixel_to_cm_ratio
    # è·ç¦»è¶Šè¿œï¼Œæ¯”ä¾‹è¶Šå°
    return pixel_to_cm_ratio * (REFERENCE_DISTANCE / distance)

# æ‰“å¼€æ‘„åƒå¤´
# cap = cv2.VideoCapture(0)
cap = cv2.VideoCapture(r"C:\Users\30907\OneDrive\æ¡Œé¢\1.mp4")

# åˆ›å»ºçª—å£
cv2.namedWindow("Height Estimation", cv2.WINDOW_NORMAL)

while True:
    ret, frame = cap.read()
    if not ret:
        print("æ— æ³•è¯»å–æ‘„åƒå¤´ï¼Œè¯·æ£€æŸ¥è¿æ¥ã€‚")
        break

    # å¦‚æœæœªæ ‡å®šï¼Œå°è¯•ä½¿ç”¨A4çº¸è¿›è¡Œæ ‡å®š
    if not calibrated:
        success, ratio, contour = calibrate_with_a4(frame)
        if success:
            pixel_to_cm_ratio = ratio
            calibrated = True
            print(f"âœ… æ ‡å®šæˆåŠŸ:1 åƒç´  â‰ˆ {pixel_to_cm_ratio:.4f} cm(å·²ä¿å­˜)")
            # åœ¨å›¾åƒä¸Šç»˜åˆ¶A4çº¸è½®å»“
            cv2.drawContours(frame, [contour], -1, (0, 255, 0), 2)
            cv2.putText(frame, "A4çº¸æ£€æµ‹æˆåŠŸ", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        else:
            cv2.putText(frame, "è¯·å°†A4çº¸æ”¾åœ¨ç”»é¢ä¸­", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

    # å¦‚æœå·²æ ‡å®šï¼Œæ‰§è¡Œäººä½“æ£€æµ‹å’Œèº«é«˜ä¼°ç®—
    if calibrated:
        results = model(frame)[0]
        frame_height = frame.shape[0]
        frame_width = frame.shape[1]

        persons = []
        for result in results.boxes:
            cls_id = int(result.cls)
            if model.names[cls_id] != "person":
                continue

            x1, y1, x2, y2 = map(int, result.xyxy[0])
            bbox_height = y2 - y1
            bbox_width = x2 - x1
            aspect_ratio = bbox_height / bbox_width

            # åˆ¤æ–­æ˜¯å¦æ˜¯å®Œæ•´äººåƒ
            if y2 >= frame_height - 20 and aspect_ratio > 1.8:
                persons.append((x1, y1, x2, y2, bbox_height, bbox_width))

        # æ˜¾ç¤ºäººæ•°ç»Ÿè®¡
        cv2.putText(frame, f"æ£€æµ‹åˆ° {len(persons)} äºº", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # æ£€æµ‹äººè„¸
        faces = detect_face(frame)
        
        # åœ¨æ¯ä¸ªäººä½“æ£€æµ‹æ¡†å†…æŸ¥æ‰¾äººè„¸
        for i, (x1, y1, x2, y2, bbox_height, bbox_width) in enumerate(persons, 1):
            # ä¼°ç®—è·ç¦»
            distance = estimate_distance(bbox_width)
            # æ ¹æ®è·ç¦»è°ƒæ•´æ¯”ä¾‹
            adjusted_ratio = adjust_height_ratio(distance)
            # è®¡ç®—èº«é«˜
            estimated_height_cm = bbox_height * adjusted_ratio

            # æ£€æŸ¥æ˜¯å¦æœ‰äººè„¸åœ¨è¿™ä¸ªäººä½“æ¡†å†…
            has_face = False
            is_you = False
            for (fx, fy, fw, fh) in faces:
                # å¦‚æœäººè„¸ä¸­å¿ƒç‚¹åœ¨äººä½“æ¡†å†…
                face_center_x = fx + fw//2
                face_center_y = fy + fh//2
                if (x1 <= face_center_x <= x2 and y1 <= face_center_y <= y2):
                    has_face = True
                    # æå–äººè„¸åŒºåŸŸ
                    face_roi = cv2.cvtColor(frame[fy:fy+fh, fx:fx+fw], cv2.COLOR_BGR2GRAY)
                    # åˆ¤æ–­æ˜¯å¦æ˜¯å‚è€ƒäººè„¸
                    if face_recognized and is_reference_face(face_roi):
                        is_you = True
                        # ç»˜åˆ¶äººè„¸æ¡†
                        cv2.rectangle(frame, (fx, fy), (fx+fw, fy+fh), (0, 0, 255), 2)
                        cv2.putText(frame, "You", (fx, fy-10),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
                    else:
                        # ç»˜åˆ¶å…¶ä»–äººè„¸æ¡†
                        cv2.rectangle(frame, (fx, fy), (fx+fw, fy+fh), (0, 255, 255), 2)
                        cv2.putText(frame, "Face", (fx, fy-10),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    break

            # ç»˜åˆ¶äººä½“è¾¹ç•Œæ¡†å’Œæ ‡ç­¾
            if is_you:
                color = (0, 0, 255)  # çº¢è‰²è¡¨ç¤ºæ˜¯ä½ 
            elif has_face:
                color = (0, 255, 255)  # é»„è‰²è¡¨ç¤ºå…¶ä»–äºº
            else:
                color = (0, 255, 0)  # ç»¿è‰²è¡¨ç¤ºæœªæ£€æµ‹åˆ°äººè„¸
                
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            
            # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            label = f"Person {i}: {estimated_height_cm:.1f} cm"
            if distance:
                label += f" (è·ç¦»: {distance:.1f} cm)"
            if is_you:
                label += " [You]"
            elif has_face:
                label += " [Face]"
            
            # ç¡®ä¿æ ‡ç­¾ä¸ä¼šè¶…å‡ºå›¾åƒè¾¹ç•Œ
            label_y = max(y1 - 10, 30)
            cv2.putText(frame, label, (x1, label_y),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

    # æ˜¾ç¤ºç”»é¢
    cv2.imshow("Height Estimation", frame)

    # æŒ‰ q é”®é€€å‡º
    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

cap.release()
cv2.destroyAllWindows() 